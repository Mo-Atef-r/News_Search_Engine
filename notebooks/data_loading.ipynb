{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de014449",
   "metadata": {},
   "source": [
    "### The 20 Newsgroups Dataset\n",
    "The 20 Newsgroups dataset is a classic and widely-used collection of documents for text classification and clustering. It consists of approximately 20,000 documents, partitioned nearly evenly across 20 different newsgroups.\n",
    "\n",
    "### What it Contains\n",
    "The dataset contains posts collected from 20 different Usenet newsgroups. These groups cover a wide range of topics, including:\n",
    "\n",
    "__Computers:__ comp.graphics, comp.sys.mac.hardware\n",
    "\n",
    "__Recreation:__ rec.sport.baseball, rec.autos\n",
    "\n",
    "__Science:__ sci.space, sci.med\n",
    "\n",
    "__Religion:__ talk.religion.misc, alt.atheism\n",
    "\n",
    "__Politics:__ talk.politics.guns, talk.politics.mideast\n",
    "\n",
    "### Format and Structure\n",
    "The dataset is primarily in __plain text__ format. Each document is a single newsgroup post. While the raw data includes headers, footers, and quoted replies, the sklearn version of the dataset can clean these elements out upon loading, as seen in the code.\n",
    "\n",
    "The data is structured as a collection of text documents and their corresponding target labels (the newsgroup they belong to). This makes it ideal for supervised learning, where a model learns to predict the correct category based on the document's content.\n",
    "\n",
    "### How it was Collected\n",
    "The dataset was originally collected by Ken Lang in 1992 and compiled from newsgroup archives. It was one of the first publicly available large-scale datasets for text classification, and its clear division into distinct categories makes it a popular benchmark for evaluating machine learning algorithms. Its well-documented nature and consistent structure have made it a staple in the field of natural language processing (NLP) research and education for decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c4bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9191e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes')) #train+test, remove metadata\n",
    "documents = newsgroups_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c87fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "print(len(documents)) #documents is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699724e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64010827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (18846, 51840)\n"
     ]
    }
   ],
   "source": [
    "#TFIDF + stopword removal + ignore terms with more than appear in more than 95% of documents and less than 2 occurences\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b5d22",
   "metadata": {},
   "source": [
    "Save files for reusage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c0450ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved Succesfully\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"D:/Data_and_AI/Projects/End-to-end/news_search_engine/data\"\n",
    "\n",
    "with open(os.path.join(data_dir, 'documents.pkl'), 'wb') as f:\n",
    "    pickle.dump(documents, f)\n",
    "    \n",
    "with open(os.path.join(data_dir, \"tfidf_matrix.pkl\"), 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix, f)\n",
    "    \n",
    "with open(os.path.join(data_dir, \"tfidf_vectorizer.pkl\"), 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(\"Data Saved Succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181188ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e9b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
